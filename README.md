# DynTopo

<p align=“center”> 
<img src=".\figures\teaser.png" width="600"> 
</p>

We propose  the **Dynamic Topological Scene Graph** (DynTopo) which introduces dynamic components and relationships into persistent topological layouts for embodied robotic autonomy.

Our framework constructs the global topological layouts from posed RGB-D inputs, encoding room-scale connectivity and large static objects (e.g., furniture), while environmental and egocentric cameras populate dynamic information with object position relations and human-object interaction patterns. A holistic unified architecture is conducted by integrating the dynamics into the global topology using semantic and spatial constraints, enabling seamless updates as the environment evolves.
An agent powered by large language models (LLMs) is employed to interpret the unified graph, infer latent task triggers, and generate executable instructions grounded in robotic affordances.

We conduct complex experiments to demonstrate DynTopo’s superior scene representation effectiveness. Real-world deployments validate the system’s practicality with a mobile manipulator: **robotics autonomously complete complex tasks with no further training or complex rewarding in a dynamic scene** as cafeteria assistant.

## Real-world Demonstration

<p align=“center”> 
<img src=".\figures\realworld.gif" width="480"> 
</p>

## Pipeline

<p align=“center”> 
<img src=".\figures\pipeline.png" width="600"> 
</p>

The scene graph construction process consists of two branches, including the topological layouts and relation generation. A united scene graph representation is generated by integrating the semantic and geometric constraints. By employing LLM as reasoning approach, the scene graph is fed as prompts, together with other context, to drive the robotic mobile manipulator to manage task sequences.

## Primary Results

<p align=“center”> 
<img src=".\figures\exp1.png" width="600"> 
<img src=".\figures\exp2.png" width="600"> 
<img src=".\figures\exp3.png" width="600"> 
</p>
